\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[top=0.9in, bottom=0.9in, left=0.9in, right=0.9in]{geometry}
\usepackage{enumitem}

\begin{document}

\begin{center}
\textbf{\Large SEAN RIGGS}\\
(980) 241-7331 â€¢ dsriggs1@gmail.com\vspace{1mm}\\
\hrule height 2pt
\vspace{2mm}
\end{center}

\noindent
\textbf{Areas of Experience}\\
Hadoop Distributed File System,  ETL, Automation, Shell Scripting, Version Control, Relational Databases, Data Visualization, Linux/Unix\\
\textbf{ETL Tools:} PySpark, Hadoop, HIVE, Airflow \\
\textbf{Databases:} Teradata, MySQL, Microsoft SQL Server\\
\textbf{Analytical Software:} SAS, Python, R, SQL, PySpark, Tableau, GitHub\\

\noindent
\textbf{WORK EXPERIENCE}\vspace{-3mm}\\
\hrule height 2pt
\vspace{2mm}
\textbf{Wells Fargo-Quantitative Associate} (June 2018-Present)
\begin{itemize}
\item Used big data technologies to construct deposit account data mart:
  \begin{itemize}[label=\textbullet]
  \item Constructed data mart from transactional level monthly deposits data, which contained billions of records with over 20 years of history. Used data mining techniques to categorize raw transaction statement descriptions into different transaction indicators. Built tableau visualizations off the data mart for analysis to assist developers.
  \end{itemize}
 \begin{itemize}[label=\textbullet]
  \item Migrated data mart process from SAS/SQL to python based big-data platform using PySpark and Hadoop. 
 PySpark best practice optimization techniques which resulted in a 50\% reduction in processing time compared to SAS/SQL.  During the ETL development, built data quality checks to validate the data at each step of the process. 
  \end{itemize}
\item Partnership with external MIT/IBM team.
 \begin{itemize}[label=\textbullet]
  \item Built deposit account dataset with 20 years of account balance and geographical data. Served as point of contact to assist in understanding the data so that they could test machine learning models to forecast balances.
  \end{itemize}
 \begin{itemize}[label=\textbullet]
  \item Worked on implementation of Graph Learning Attention Mechanism (GLAM) model built by the MIT/IBM team for predicting the growth rate of account balances. The model used a modified Graph Attention Network to create a sparse graph structure based on geographical features of accounts. It was implemented using PyTorch and PyTorch Geometric libraries.
  \end{itemize}
\item Linked internal Wholesale accounts to BEA consumer spending by NAICS Subsector to help inform management of risk of exceeding asset cap. Used Tableau visualizations to highlight estimated impact to Wells Fargo balances if consumer spending exceeded pre-COVID19 levels.
\item Built datasets from transactional level data and developed features to aid model developers in identifying accounts in troubled positions and the risks of aggregate losses from those accounts. Developed dashboard visualizations from transactional data using Tableau and regularly presented findings to management and relevant stakeholders.
\end{itemize}

\noindent
\textbf{Bank of America-Quantitative Finance Analyst} (August 2017-April 2018)
\begin{itemize}
\item Responsible for running as many as 20 statistical tests as part of the validation process for logistic regression credit scorecard models.  Key responsibilities include modifying and developing SAS Macros to perform key statistical tests to evaluate model accuracy, discriminatory power, and sensitivity to changes in model parameters.  
\item Worked with developers to understand complex methodologies and data manipulations such as the creation and replication of pseudo default datasets used for scorecard modeling.
\item Developed challenger models with alternative inputs and data manipulations to provide effective challenge to models submitted by developers.
\item Performed quarterly ongoing monitoring for 10 credit scorecard models, and documented results using Latex for typesetting.
\end{itemize}

\noindent
\textbf{Wells Fargo-Forecast Analyst /Analytic Consultant} (September 2015-August 2017) 
\begin{itemize}
\item Used SAS Macro language programming to quickly loop through multiple forecasting models to efficiently back-test alternative predictive models. Used both multiple regression analysis, and Box-Jenkins time series analysis to select the best model.  Used automated code to back-test challenger models using cross-validation, and holdout sample.  Presented findings to management and business partners.
\item Re-developed Bankruptcy inflow forecasting model using multiple regression model with seasonal adjustment that resulted in forecasting error being reduced by more than 50% for both short and long-term forecasts. 
\item Took initiative to integrate R functionality within the SAS environment through Proc IML.  Educated forecast team on the capabilities of using R and SAS together, and lead effort to automate forecasts using user-built R functions.
\item Worked with other teams to employ use of Box-Jenkins Methodology to identify seasonality in time series and select appropriate ARIMA forecasting model specification.
\item Automated forecasting models and KPI metrics using both SAS language, as well as advanced excel VLOOKUP and match index functions.
\item Developed Service Release forecasting process and expanded it from three line of businesses to encompass all of default servicing.  Communicated regularly with forecast owners for each line of business.
\item Leveraged SQL server database to automate manual reporting tasks that had previously been done in Excel by building forecast history SQL table to automatically update KPI accuracy metrics. Developed complex SQL queries using subqueries to pull data from multiple data sources, and perform data transformations.
\item Developed 10 ad hoc forecasts across the Bankruptcy business in support of capacity tool development to help senior leaders, and business partners to better understand the key drivers of the Bankruptcy forecast.
\item Responsible for tracking forecasting accuracy across multiple lines of business, and using these accuracy metrics to determine where improvements in forecasting methodology can be made. Developed KPI metrics to track accuracy using various metrics, and time intervals. 
\end{itemize}

\noindent
\textbf{PERSONAL PROJECTS (in-progress)}\vspace{-3mm}\\
\hrule height 2pt
\vspace{2mm}
\textit{Fantasy Baseball: Using machine learning techniques to predict Major League Baseball player performance} (GitHub: https://github.com/dsriggs1/Baseball\_Project)
\begin{itemize}
\item Designed object-oriented programming module in python based on the optimized Polars library for data exploration/transformations. Python module uses object-oriented programming techniques to group common classification and regression prediction algorithms to increase code re-usability and reduce repetition.
\item Used MySQL database for data analysis and feature engineering of dataset with over 12 million observations and 200 columns from 1952-present; data is updated each season.
\item Input features were created as statistical player performance inputs based on rolling time periods and segmented by batter vs pitcher matchup.
\end{itemize}

\noindent
\textit{Rcpp library: Using c++ to write more efficient R functions} (GitHub: https://github.com/dsriggs1/Rcpp-Library)
\begin{itemize}
\item Used Rcpp package to write optimized common rolling functions for data analysis. Used object-oriented programming principles like inheritance to reduce code repetition.
\end{itemize}

\noindent
\textbf{EDUCATION}\vspace{-3mm}\\
\hrule height 2pt
\vspace{2mm}
\textit{Master of Science in Economics}, UNC Charlotte; Charlotte, NC (2014-2016)
\begin{itemize}
\item Graduate Econometrics, Advanced Business Forecasting, Advanced Macroeconomics, Financial Econometrics, Financial Management 
\item Awarded merit based graduate assistantship
\end{itemize}

\noindent
\textit{Bachelor of Science in Economics}, NC State University; Raleigh, NC (2009-2014)
\begin{itemize}
\item Graduated Cum Laude
\end{itemize}


\end{document}

