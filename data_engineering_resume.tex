\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[top=0.9in, bottom=0.9in, left=0.9in, right=0.9in]{geometry}
\usepackage{enumitem}

\begin{document}

\begin{center}
\textbf{\Large SEAN RIGGS}\\
(980) 241-7331 â€¢ dsriggs1@gmail.com\vspace{1mm}\\
\hrule height 2pt
\vspace{2mm}
\end{center}

\noindent
\textbf{Areas of Experience:}
HDFS,  ETL, Automation, Shell Scripting, Version Control, Data Visualization, Unix\\
\textbf{ETL Tools:} PySpark, Hadoop, HIVE, Airflow \\
\textbf{Databases:} Teradata, MySQL, Microsoft SQL Server\\
\textbf{Analytical Software:} SAS, Python, R, SQL, PySpark, Tableau, GitHub\\

\noindent
\textbf{WORK EXPERIENCE}\vspace{-3mm}\\
\hrule height 2pt
\vspace{2mm}
\textbf{Wells Fargo-Quantitative Associate} (June 2018-Present)
\begin{itemize}
\item Used big data technologies to construct deposit account data mart:
  \begin{itemize}[label=\textbullet]
  \item Developed a data mart from 20 years of transactional data with billions of records, used data mining to categorize transactions and creating Tableau visualizations for developer analysis.
  \end{itemize}
 \begin{itemize}[label=\textbullet]
  \item Optimized data mart migration from SAS/SQL to a Python-based big-data platform using PySpark and Hadoop, achieving a 50\% reduction in processing time and incorporating data quality checks during ETL development.
  \end{itemize}
 \begin{itemize}[label=\textbullet]
  \item Automated ETL process using Airflow workflow that processed billions of transactions monthly across 5 different data sources reducing manual workload by 20\%.
  \end{itemize}
 \begin{itemize}[label=\textbullet]
  \item Designed and implemented Hive views to streamline data aggregation from large datasets, and utilized Tableau to visualize time-based trends, enhancing data-driven decision-making processes.
  \end{itemize}
\item Partnership with external MIT/IBM team.
 \begin{itemize}[label=\textbullet]
  \item Built deposit account dataset with 20 years of account balance and geographical data. Served as point of contact to assist in understanding the data so that they could test machine learning models to forecast balances.
  \end{itemize}
 \begin{itemize}[label=\textbullet]
  \item Implemented MIT/IBM's Graph Learning Attention Mechanism (GLAM) using PyTorch libraries for account balance growth prediction, utilizing geographic features in a sparse graph network.
  \end{itemize}
\item Linked internal Wholesale accounts to BEA consumer spending by NAICS Subsector to help inform management of risk of exceeding asset cap. Used Tableau visualizations to highlight estimated impact to Wells Fargo balances if consumer spending exceeded pre-COVID19 levels.
\item Developed datasets and features from transactional data to identify high-risk accounts and potential aggregate losses, creating Tableau dashboards for management presentations and stakeholder engagement.
\end{itemize}

\noindent
\textbf{Bank of America-Quantitative Finance Analyst} (August 2017-April 2018)
\begin{itemize}
\item Responsible for running as many as 20 statistical tests as part of the validation process for logistic regression credit scorecard models.  Key responsibilities include modifying and developing SAS Macros to perform key statistical tests to evaluate model accuracy, discriminatory power, and sensitivity to changes in model parameters.  
\item Worked with developers to understand complex methodologies and data manipulations such as the creation and replication of pseudo default datasets used for scorecard modeling.
\item Developed challenger models with alternative inputs and data manipulations to provide effective challenge to models submitted by developers.
\item Performed quarterly ongoing monitoring for 10 credit scorecard models, and documented results using Latex for typesetting.
\end{itemize}

\noindent
\textbf{Wells Fargo-Forecast Analyst /Analytic Consultant} (September 2015-August 2017) 
\begin{itemize}
\item Used SAS Macro language programming to quickly loop through multiple forecasting models to efficiently back-test alternative predictive models. Used both multiple regression analysis, and Box-Jenkins time series analysis to select the best model.  Used automated code to back-test challenger models using cross-validation, and holdout sample.  Presented findings to management and business partners.
\item Re-developed Bankruptcy inflow forecasting model using multiple regression model with seasonal adjustment that resulted in forecasting error being reduced by more than 50% for both short and long-term forecasts. 
\item Took initiative to integrate R functionality within the SAS environment through Proc IML.  Educated forecast team on the capabilities of using R and SAS together, and lead effort to automate forecasts using user-built R functions.
\item Worked with other teams to employ use of Box-Jenkins Methodology to identify seasonality in time series and select appropriate ARIMA forecasting model specification.
\item Automated forecasting models and KPI metrics using both SAS language, as well as advanced excel VLOOKUP and match index functions.
\item Developed Service Release forecasting process and expanded it from three line of businesses to encompass all of default servicing.  Communicated regularly with forecast owners for each line of business.
\item Leveraged SQL server database to automate manual reporting tasks that had previously been done in Excel by building forecast history SQL table to automatically update KPI accuracy metrics. Developed complex SQL queries using subqueries to pull data from multiple data sources, and perform data transformations.
\item Developed 10 ad hoc forecasts across the Bankruptcy business in support of capacity tool development to help senior leaders, and business partners to better understand the key drivers of the Bankruptcy forecast.
\item Responsible for tracking forecasting accuracy across multiple lines of business, and using these accuracy metrics to determine where improvements in forecasting methodology can be made. Developed KPI metrics to track accuracy using various metrics, and time intervals. 
\end{itemize}

\noindent
\textbf{PERSONAL PROJECTS (in-progress)}\vspace{-3mm}\\
\hrule height 2pt
\vspace{2mm}
\textit{Fantasy Baseball: Using machine learning techniques to predict Major League Baseball player performance} (GitHub: https://github.com/dsriggs1/Baseball\_Project)
\begin{itemize}
\item Designed object-oriented programming module in python based on the optimized Polars library for data exploration/transformations. Python module uses object-oriented programming techniques to group common classification and regression prediction algorithms to increase code re-usability and reduce repetition.
\item Used MySQL database for data analysis and feature engineering of dataset with over 12 million observations and 200 columns from 1952-present; data is updated each season.
\item Input features were created as statistical player performance inputs based on rolling time periods and segmented by batter vs pitcher matchup.
\end{itemize}

\noindent
\textit{Rcpp library: Using c++ to write more efficient R functions} (GitHub: https://github.com/dsriggs1/Rcpp-Library)
\begin{itemize}
\item Used Rcpp package to write optimized common rolling functions for data analysis. Used object-oriented programming principles like inheritance to reduce code repetition.
\end{itemize}

\noindent
\textbf{EDUCATION}\vspace{-3mm}\\
\hrule height 2pt
\vspace{2mm}
\textit{Master of Science in Economics}, UNC Charlotte; Charlotte, NC (2014-2016)
\begin{itemize}
\item Graduate Econometrics, Advanced Business Forecasting, Advanced Macroeconomics, Financial Econometrics, Financial Management 
\item Awarded merit based graduate assistantship
\end{itemize}

\noindent
\textit{Bachelor of Science in Economics}, NC State University; Raleigh, NC (2009-2014)
\begin{itemize}
\item Graduated Cum Laude
\end{itemize}


\end{document}

