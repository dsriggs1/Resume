\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[top=0.75in, bottom=0.75in, left=0.75in, right=0.75in]{geometry}
\usepackage{enumitem}

\begin{document}

\begin{center}
\textbf{\Large SEAN RIGGS}\\
(980) 241-7331 â€¢ dsriggs1@gmail.com\vspace{1mm}\\
\hrule height 2pt
\vspace{2mm}
\end{center}

\noindent
\textbf{Areas of Experience:}
HDFS,  ETL, Automation, Shell Scripting, Version Control, Data Visualization, Unix\\
\textbf{ETL Tools:} PySpark, Hadoop, HIVE, Airflow \\
\textbf{Databases:} Teradata, MySQL, Microsoft SQL Server\\
\textbf{Analytical Software:} SAS, Python, R, SQL, PySpark, Tableau, GitHub\\

\noindent
\textbf{WORK EXPERIENCE}\vspace{-3mm}\\
\hrule height 2pt
\vspace{2mm}
\noindent
\begin{tabular}{p{0.7\textwidth} r}
   \textbf{Wells Fargo} & \textbf{June 2018-Present }\\
    \textbf{Quantitative Analytics Specialist - Data Engineer} & \\    
\end{tabular}
\begin{itemize}
  \item Developed a data mart from 20 years of transactional data with billions of records, used data mining to categorize transactions and creating Tableau visualizations for developer analysis.
  \item Optimized data mart migration from SAS/SQL to a Python-based big-data platform using PySpark and Hadoop, achieving a 50\% reduction in processing time and incorporating data quality checks during ETL development.
  \item Automated ETL process using Airflow workflow that processed billions of transactions monthly across 5 different data sources reducing manual workload by 20\%.
  \item Designed and implemented Hive views to streamline data aggregation from large datasets, and utilized Tableau to visualize time-based trends, enhancing data-driven decision-making processes.
  \item Constructed a 20-year deposit account dataset with geographical data for the MIT/IBM team, serving as the point of contact to support their machine learning model development for balance forecasting.
  \item Implemented MIT/IBM's Graph Learning Attention Mechanism (GLAM) using PyTorch libraries for account balance growth prediction, utilizing geographic features in a sparse graph network.
  \item Linked internal Wholesale accounts to BEA consumer spending by NAICS Subsector to help inform management of risk of exceeding asset cap. Used Tableau visualizations to highlight estimated impact to Wells Fargo balances if consumer spending exceeded pre-COVID19 levels.
  \item Developed datasets and features from transactional data to identify high-risk accounts and potential aggregate losses, creating Tableau dashboards for management presentations and stakeholder engagement.
\end{itemize}

\noindent
\begin{tabular}{p{0.7\textwidth} r}
   \textbf{Bank of America} & \textbf{August 2017-April 2018}\\
    \textbf{Quantitative Finance Analyst} & \\    
\end{tabular}
\begin{itemize}
\item Conducted up to 20 statistical tests for validating logistic regression credit scorecard models, and refined SAS Macros to assess model accuracy, discriminatory power, and sensitivity to parameter changes.
\item Worked with developers to understand complex methodologies and data manipulations such as the creation and replication of pseudo default datasets used for scorecard modeling.
\item Developed challenger models with alternative inputs and data manipulations to provide effective challenge to models submitted by developers.
\item Performed quarterly ongoing monitoring for 10 credit scorecard models, and documented results using Latex for typesetting.
\end{itemize}

\noindent
\begin{tabular}{p{0.7\textwidth} r}
   \textbf{Wells Fargo} & \textbf{September 2015-August 2017}\\
    \textbf{Forecast Analyst /Analytic Consultant} & \\    
\end{tabular}

\begin{itemize}
\item Streamlined forecasting model evaluation using SAS Macro programming, incorporating multiple regression and Box-Jenkins methodology, automated cross-validation and holdout sample tests, Presented key findings to management and business partners.
\item Re-developed Bankruptcy inflow forecasting model using multiple regression model with seasonal adjustment that resulted in forecasting error being reduced by more than 50\% for both short and long-term forecasts. 
\item Took initiative to integrate R functionality within the SAS environment through Proc IML.  Educated forecast team on the capabilities of using R and SAS together, and lead effort to automate forecasts using user-built R functions.
\item Worked with other teams to employ use of Box-Jenkins Methodology to identify seasonality in time series and select appropriate ARIMA forecasting model specification.
\item Automated forecasting models and KPI metrics using both SAS language, as well as advanced excel VLOOKUP and match index functions.
\item Developed Service Release forecasting process and expanded it from three line of businesses to encompass all of default servicing.  Communicated regularly with forecast owners for each line of business.
\item Streamlined Excel reporting by creating a SQL server database for auto-updating KPI metrics, using advanced SQL queries for data aggregation and transformation.
\item Developed 10 ad hoc forecasts across the Bankruptcy business in support of capacity tool development to help senior leaders, and business partners to better understand the key drivers of the Bankruptcy forecast.
\item Responsible for tracking forecasting accuracy across multiple lines of business, and using these accuracy metrics to determine where improvements in forecasting methodology can be made. Developed KPI metrics to track accuracy using various metrics, and time intervals. 
\end{itemize}

\noindent
\textbf{PERSONAL PROJECTS (in-progress)}\vspace{-3mm}\\
\hrule height 2pt
\vspace{2mm}
\textit{Fantasy Baseball: Using machine learning techniques to predict Major League Baseball player performance} (GitHub: https://github.com/dsriggs1/Baseball\_Project)
\begin{itemize}
\item Designed object-oriented programming module in python based on the optimized Polars library for data exploration/transformations. Python module uses object-oriented programming techniques to group common classification and regression prediction algorithms to increase code re-usability and reduce repetition.
\item Used MySQL database for data analysis and feature engineering of dataset with over 12 million observations and 200 columns from 1952-present; data is updated each season.
\item Input features were created as statistical player performance inputs based on rolling time periods and segmented by batter vs pitcher matchup.
\end{itemize}

\noindent
\textit{Rcpp library: Using c++ to write more efficient R functions} (GitHub: https://github.com/dsriggs1/Rcpp-Library)
\begin{itemize}
\item Used Rcpp package to write optimized common rolling functions for data analysis. Used object-oriented programming principles like inheritance to reduce code repetition.
\end{itemize}

\noindent
\textbf{EDUCATION}\vspace{-3mm}\\
\hrule height 2pt
\vspace{2mm}
\textit{Master of Science in Economics}, UNC Charlotte; Charlotte, NC (2014-2016)
\begin{itemize}
\item Graduate Econometrics, Advanced Business Forecasting, Advanced Macroeconomics, Financial Econometrics, Financial Management 
\item Awarded merit based graduate assistantship
\end{itemize}

\noindent
\textit{Bachelor of Science in Economics}, NC State University; Raleigh, NC (2009-2014)
\begin{itemize}
\item Graduated Cum Laude
\end{itemize}


\end{document}

